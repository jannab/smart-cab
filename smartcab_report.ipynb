{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4: Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Smartcab to Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the project “Train a Smartcab to Drive”, a smartcab operating in an idealized grid-like city is given. There are traffic lights at each intersection and other cars present. The smartcab gets a reward for obeying traffic rules and a penalty for not obeying traffic rules or causing an accident. Goal of this project is to implement a learning agent for the smartcab that should learn an optimal policy for driving on city roads, obeying traffic rules correctly, and trying to reach the destination within a goal time based on the rewards and penalties it gets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Implement a basic driving agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first step, I let the smart cab take a random action out of the possibilities of:\n",
    "\n",
    "- doing nothing (state ‘None’)\n",
    "- driving forward (state ‘forward’)\n",
    "- turning left (state ‘left’)\n",
    "- turning right (state ‘right’)\n",
    "\n",
    "In this step, the smartcab does not learn from the results of it’s actions and has unlimited time to reach the goal.\n",
    "\n",
    "I put the printed output of my test run into `'output_first_text.txt'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_in_my_text(file_name, print_lines):\n",
    "    with open (file_name, \"r\") as myfile:\n",
    "        output = myfile.read()\n",
    "    count_reached = output.count('Primary agent has reached destination!')\n",
    "    count_aborted = output.count('Trial aborted.')\n",
    "    lines = output.split('\\n')\n",
    "    print \"Rate of reaching the destination {}\".format(count_reached*1.0/(count_reached + count_aborted))\n",
    "    if print_lines == True:\n",
    "        print \"Last 3 lines:\"\n",
    "        for line in lines[-3:]:\n",
    "            print line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate of reaching the destination 0.19801980198\n"
     ]
    }
   ],
   "source": [
    "read_in_my_text(\"outputs/out_1_1.txt\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate of reaching the destination 0.25\n"
     ]
    }
   ],
   "source": [
    "read_in_my_text(\"outputs/out_1_2.txt\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate of reaching the destination 0.2\n"
     ]
    }
   ],
   "source": [
    "read_in_my_text(\"outputs/out_1_3.txt\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate of reaching the destination 0.23\n"
     ]
    }
   ],
   "source": [
    "read_in_my_text(\"outputs/out_1_4.txt\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate of reaching the destination 0.16\n"
     ]
    }
   ],
   "source": [
    "read_in_my_text(\"outputs/out_1_5.txt\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The smartcab reaches the destination in time in around 20% of the trials. If the smartcab reaches the destination is a question of chance. The actions are chosen randomly, independent of the destination and the received rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Identify and update state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the states, I chose a tupel of `'next_waypoint'`, `'light'`, `'oncoming'` and `'left'`.\n",
    "\n",
    "Without the `'next_waypoint'`, our agent wouldn't know where to go next and reaching it's destination would be a coincedence.\n",
    "\n",
    "Without `'light'`, the agent would ignore if the light is red or green and therefore not be able to follow the traffic rules.\n",
    "\n",
    "Without `'oncoming'`, the agent wouldn't be able to differentiate in a situation where the traffic light shows green and he wants to turn left.\n",
    "\n",
    "Without `'left'`, the agent wouldn't be able to differentiate in a situation where the traffic light shows red and he wants to turn right.\n",
    "\n",
    "I didn't take `'right'` into account, because no matter what our smartcab is going to do, the traffic from the right side has no influence on it.\n",
    "\n",
    "I also didn't take the deadline into account. This doesn't help our agent to make the right decisions to follow the traffic rules. For example, if our agent is at an intersection, the light is red and he wants to go forward, the deadline would not give him any information on how to react. It would increase the state space without a good reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Implement Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step I implemented the q-learning algorithm with a learning rate of 0.5 and a discount factor of 0.5. The next action is always chosen based on the best estemate based on the current state. I didn't use an exploration rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate of reaching the destination 0.911764705882\n",
      "Last 3 lines:\n",
      "Environment.act(): Primary agent has reached destination!\n",
      "Environment.act(): Primary agent has reached destination!\n",
      "Environment.act(): Primary agent has reached destination!\n"
     ]
    }
   ],
   "source": [
    "read_in_my_text(\"outputs/out_3_1.txt\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate of reaching the destination 0.93137254902\n",
      "Last 3 lines:\n",
      "Environment.act(): Primary agent has reached destination!\n",
      "Environment.act(): Primary agent has reached destination!\n",
      "Environment.act(): Primary agent has reached destination!\n"
     ]
    }
   ],
   "source": [
    "read_in_my_text(\"outputs/out_3_2.txt\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate of reaching the destination 0.06\n",
      "Last 3 lines:\n",
      "Environment.step(): Primary agent ran out of time! Trial aborted.\n",
      "Environment.step(): Primary agent ran out of time! Trial aborted.\n",
      "Environment.step(): Primary agent ran out of time! Trial aborted.\n"
     ]
    }
   ],
   "source": [
    "read_in_my_text(\"outputs/out_3_3.txt\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate of reaching the destination 0.970297029703\n",
      "Last 3 lines:\n",
      "Environment.act(): Primary agent has reached destination!\n",
      "Environment.act(): Primary agent has reached destination!\n",
      "Environment.act(): Primary agent has reached destination!\n"
     ]
    }
   ],
   "source": [
    "read_in_my_text(\"outputs/out_3_4.txt\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate of reaching the destination 0.950980392157\n",
      "Last 3 lines:\n",
      "Environment.act(): Primary agent has reached destination!\n",
      "Environment.act(): Primary agent has reached destination!\n",
      "Environment.act(): Primary agent has reached destination!\n"
     ]
    }
   ],
   "source": [
    "read_in_my_text(\"outputs/out_3_5.txt\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the smartcab reaches the destination quite often in time. There seem to be some cases, when the smartcab get's stuck and doesn't reach the destination in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Enhance the driving agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I played around with the learning rate and calculated it using the time step. I stayed with a discount factor of 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_results(file_name):\n",
    "    results = pd.read_csv(file_name, header = None, names = [\"Learning Rate\", \"Discount Factor\", \"Exploration Rate\", \"Initial Deadline\", \"Steps Needed\", \"Total Reward\", \"Trial Reward\"])\n",
    "    # delete first row, no values\n",
    "    results = results[1:]\n",
    "    #display(results[-10:])\n",
    "    #display(results)\n",
    "    return results\n",
    "\n",
    "def get_info(input, trial_number):\n",
    "    reached = 0\n",
    "    for i in range(len(input)-10,len(input)):\n",
    "        if input[\"Steps Needed\"][i] <= input[\"Initial Deadline\"][i]:\n",
    "            reached += 1\n",
    "    \n",
    "    print \"Results for last 10 Trials:\"\n",
    "    print \"Mean Reward: {}\".format(input[\"Trial Reward\"][-10:].mean())\n",
    "    print \"Times Destination reached: {}\".format(reached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For finding the best learning rate, I started with a value of 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_in_attempts(number):\n",
    "    for i in range (1,6):\n",
    "        result = get_results('learning_rate_{}_{}.csv'.format(number, i))\n",
    "        get_info(result, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for last 10 Trials:\n",
      "Mean Reward: 20.0\n",
      "Times Destination reached: 10\n",
      "Results for last 10 Trials:\n",
      "Mean Reward: 20.3\n",
      "Times Destination reached: 10\n",
      "Results for last 10 Trials:\n",
      "Mean Reward: 2.75\n",
      "Times Destination reached: 10\n",
      "Results for last 10 Trials:\n",
      "Mean Reward: 22.2\n",
      "Times Destination reached: 10\n",
      "Results for last 10 Trials:\n",
      "Mean Reward: 14.3\n",
      "Times Destination reached: 10\n"
     ]
    }
   ],
   "source": [
    "first_attempt = read_in_attempts(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was pretty happy with my results, but played with other values, too. For exmple, for a learning rate of 0.75 I got the following results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for last 10 Trials:\n",
      "Mean Reward: 22.15\n",
      "Times Destination reached: 10\n",
      "Results for last 10 Trials:\n",
      "Mean Reward: 20.75\n",
      "Times Destination reached: 10\n",
      "Results for last 10 Trials:\n",
      "Mean Reward: 22.4\n",
      "Times Destination reached: 10\n",
      "Results for last 10 Trials:\n",
      "Mean Reward: 19.95\n",
      "Times Destination reached: 10\n",
      "Results for last 10 Trials:\n",
      "Mean Reward: 23.75\n",
      "Times Destination reached: 10\n"
     ]
    }
   ],
   "source": [
    "second_attempt = read_in_attempts(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was not better in reaching the goals, but with the mean reward of the last 10 trials. I tried using the total steps the agent did. I wanted to the learning rate to decrease with increasing total steps. The results of just implementing a learning rate of (1.0 / (t + 5)) + 0.75 which will decrease with increasing t are as follows. I needed the (t + 5) to not get over 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for last 10 Trials:\n",
      "Mean Reward: 20.9\n",
      "Times Destination reached: 10\n",
      "Results for last 10 Trials:\n",
      "Mean Reward: 22.35\n",
      "Times Destination reached: 10\n",
      "Results for last 10 Trials:\n",
      "Mean Reward: 19.2\n",
      "Times Destination reached: 10\n",
      "Results for last 10 Trials:\n",
      "Mean Reward: 22.05\n",
      "Times Destination reached: 10\n",
      "Results for last 10 Trials:\n",
      "Mean Reward: 23.05\n",
      "Times Destination reached: 10\n"
     ]
    }
   ],
   "source": [
    "third_attempt = read_in_attempts(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are very good, but I thought what might happen if I have a learning rate getting over 1.0 in the beginning. A learning rate of (1.0 / t) + 0.75. That would mean the agent would consider only most recent information in the first four steps, because then the learning rate is 1.0 or above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for last 10 Trials:\n",
      "Mean Reward: 24.15\n",
      "Times Destination reached: 10\n",
      "Results for last 10 Trials:\n",
      "Mean Reward: 23.35\n",
      "Times Destination reached: 10\n",
      "Results for last 10 Trials:\n",
      "Mean Reward: 23.55\n",
      "Times Destination reached: 10\n",
      "Results for last 10 Trials:\n",
      "Mean Reward: 24.3\n",
      "Times Destination reached: 10\n",
      "Results for last 10 Trials:\n",
      "Mean Reward: 19.75\n",
      "Times Destination reached: 10\n"
     ]
    }
   ],
   "source": [
    "forth_attempt = read_in_attempts(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gave me even higher mean rewards in the last 10 trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the changes I made to the learning rate, I changed the initialization from my q table from initializing it with zeros to using random values between 0 and 4. With my first attempt the smartcab got stuck most of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is one example output of states, when the agent acts suboptimally:\n",
    "`Trial 90\n",
    "Go: left, lights: green, oncoming: None, left: None, action: right\n",
    "Go: left, lights: green, oncoming: None, left: None, action: right\n",
    "Trial 91\n",
    "Go: forward, lights: green, oncoming: None, left: None, action: right\n",
    "Trial 92\n",
    "Trial 93\n",
    "Go: forward, lights: green, oncoming: None, left: None, action: right\n",
    "Trial 94\n",
    "Go: forward, lights: green, oncoming: None, left: None, action: right\n",
    "Go: right, lights: green, oncoming: right, left: None, action: forward\n",
    "Trial 95\n",
    "Trial 96\n",
    "Trial 97\n",
    "Go: forward, lights: green, oncoming: None, left: forward, action: left\n",
    "Trial 98\n",
    "Go: right, lights: green, oncoming: None, left: None, action: forward\n",
    "Trial 99\n",
    "Go: right, lights: green, oncoming: None, left: None, action: forward\n",
    "Trial 100`\n",
    "\n",
    "It seems that in some cases, the agent chooses a wrong direction to go. In trial 90, it chose twice to turn right and followed the traffic rules, but it should have gone left.\n",
    "\n",
    "The only time the driving agent does not follow the traffic rules is in trial 97.\n",
    "\n",
    "It would be possible to change the driving agent to only go in the desired direction or wait, but that would minimize exploration.\n",
    "\n",
    "My final agent finds a close to optimal policy. As seen above it usually reaches the destination in time in the last 10 trials and it usually only gets penalties for not going in the right direction, but rarely for not following the traffic rules."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
